{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zchgdf75vC0D"
   },
   "source": [
    "### Colaboratory Setup\n",
    "#### *ignore if not in colab environment\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "colab_type": "code",
    "id": "M7Ewbdkz7pAa",
    "outputId": "743d5e8b-f92a-4575-fbcd-f274bdf590c1"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "# drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GqY0ZpHj79i2"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# os.chdir(\"/content/gdrive/My Drive/4995_trading\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NFxa6Ket53Ki"
   },
   "source": [
    "# Environment Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ufRmdxep2_s8"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "\n",
    "DATA_DIR = 'data/'\n",
    "\n",
    "class EquityEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "    def __init__(self, \n",
    "                 principal=1000000, \n",
    "                 use_cost=False,\n",
    "                 split_data=False,\n",
    "                 asset_num=3, \n",
    "                 transaction_ratio=0.0002,\n",
    "                 episode_length=120):\n",
    "        \"\"\" \n",
    "        .t: int idx of df\n",
    "        .google/.amazon/.msft: [[Open, Close, High, Low, Volume]]\n",
    "        .holdings: {google_pos: int, amazon_pos: int, msft_pos: int} \n",
    "        ._setup(): helper function that cleans data and initializes dfs\n",
    "        .position: double = balance (double) + total of all pos. (double) \n",
    "        \"\"\"  \n",
    "        \n",
    "        self.principal = principal\n",
    "        self.balance = principal\n",
    "        self.use_cost = use_cost\n",
    "        self.position = None\n",
    "        self.pnl = 0\n",
    "        self.transaction_ratio = transaction_ratio\n",
    "        self.asset_num = asset_num\n",
    "        \n",
    "        # State/Action Spaces\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=3, shape=(5*asset_num+asset_num,), dtype='float32')\n",
    "        self.action_space = gym.spaces.Box(-2, 2, shape=(asset_num,), dtype='float32')\n",
    "\n",
    "        # Date idx\n",
    "        self.t = None  \n",
    "        self.start_idx = None\n",
    "        self.end_idx = None\n",
    "        \n",
    "        # Dataframes\n",
    "        self.states = None\n",
    "        self.close_prices = None\n",
    "        self.open_prices = None\n",
    "        self.dates = None\n",
    "        \n",
    "        # Initializes dfs\n",
    "        self._setup()\n",
    "        \n",
    "        # Training Params\n",
    "        self.split_data = split_data\n",
    "        period = len(self.close_prices)\n",
    "        self.test_length = round(period/500)*100 #7200\n",
    "        self.train_period = np.arange(0, period-self.test_length) #0 to 28800\n",
    "        self.test_period = np.arange(period-self.test_length, period) # 28800 to 36000\n",
    "\n",
    "        print('-- Environment Created --')\n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        return: [position + google_t + amazon_t + msft_t]\n",
    "        \"\"\"\n",
    "        if self.split_data:\n",
    "            self.start_idx = max(0, random.choice(TRAIN_PERIOD) - EPISODE_LENGTH)\n",
    "            self.end_idx = self.start_idx + EPISODE_LENGTH\n",
    "        else:\n",
    "            self.start_idx = 0\n",
    "            self.end_idx = 36000\n",
    "        self.t = self.start_idx\n",
    "        self.balance = self.principal\n",
    "        self.position = np.array([0.0] * self.asset_num)\n",
    "        self.pnl = 0\n",
    "        return self._get_state(self.t)\n",
    "        \n",
    "    def step(self, action: list):\n",
    "        \"\"\"\n",
    "        action: [new_google_pos, new_amazon, new_msft_pos]\n",
    "        return: \n",
    "                <obs>   : [new_pos, google_t, amazon_t, msft_t]\n",
    "                <reward>: double, capital_gain - transaction_cost\n",
    "                <done>  : bool\n",
    "                <info>  : {\n",
    "                           'date': dateobj, \n",
    "                           'transaction_cost': double\n",
    "                           'capital_gain': double ,\n",
    "                           'previous_close': double,\n",
    "                           'current_close': double\n",
    "                          } \n",
    "        \"\"\"\n",
    "        self.t = self.t + 1\n",
    "\n",
    "        # Done\n",
    "        done = True if self.t >= self.end_idx else False\n",
    "        \n",
    "        # Reward \n",
    "        reward, info = self._get_reward(action)\n",
    "        \n",
    "        # Next State\n",
    "        next_state = self._get_state(self.t)\n",
    "        \n",
    "        return next_state, reward, done, info\n",
    "    \n",
    "    def render(self, mode='human', close=False):\n",
    "        return NotImplemented\n",
    "    \n",
    "    def _get_close_prices(self, t):\n",
    "        return np.array(self.close_prices.iloc[t].tolist())\n",
    "    \n",
    "    def _get_state(self, t):\n",
    "        return np.concatenate([self.position,\n",
    "                               self.states.iloc[t].tolist()])\n",
    "    \n",
    "    def _get_reward(self, action):\n",
    "        action = np.array(action)\n",
    "        \n",
    "        # Positions (dollar neutral): long/short pos same\n",
    "        old_position = self.position\n",
    "        new_position = action - action.mean()\n",
    "        \n",
    "        # Clipping weights\n",
    "        for i in range(self.asset_num):\n",
    "            if new_position[i] > 1:\n",
    "                new_position[i] = 1\n",
    "            elif new_position[i] < -1:\n",
    "                new_position[i] = -1\n",
    "        \n",
    "        # Close Prices\n",
    "        previous_close = self._get_close_prices(self.t-1)\n",
    "        current_close = self._get_close_prices(self.t)\n",
    "        \n",
    "        # Intermediate Reward Calculations\n",
    "        capital_gain = np.dot(new_position, (current_close - previous_close) / previous_close) * self.principal\n",
    "        transaction_cost = (np.absolute(new_position - old_position).sum() * self.transaction_ratio * self.principal) if self.use_cost else 0\n",
    "        \n",
    "        # Reward\n",
    "        reward = capital_gain - transaction_cost\n",
    "        self.pnl += reward\n",
    "        self.position = new_position\n",
    "        \n",
    "        # Debugging Info\n",
    "        info = {'date': self.dates.iloc[self.t],\n",
    "                'transaction_cost': transaction_cost,\n",
    "                'capital_gain': capital_gain,\n",
    "                'previous_close': previous_close,\n",
    "                'current_close': current_close}\n",
    "        \n",
    "        return reward, info\n",
    "    \n",
    "    def _setup(self):\n",
    "        states_df = pd.read_csv(DATA_DIR + \"/state.csv\")\n",
    "        prices_df = pd.read_csv(DATA_DIR + \"/price.csv\")\n",
    "        self.states = states_df[['open_gg', 'close_gg', 'high_gg', 'low_gg', 'volume_gg',\n",
    "                       'open_am', 'close_am', 'high_am', 'low_am', 'volume_am',\n",
    "                       'open_ms', 'close_ms', 'high_ms', 'low_ms', 'volume_ms']]\n",
    "        self.close_prices = prices_df[['close_gg', 'close_am', 'close_ms']]\n",
    "        self.open_prices = prices_df[['open_gg', 'open_am', 'open_ms']]\n",
    "        self.dates = states_df[['Dates']]\n",
    "        print('-- Data Loaded --')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 220
    },
    "colab_type": "code",
    "id": "b3ObAfv_Omp0",
    "outputId": "72af4179-d717-4263-b502-db08f146a960"
   },
   "outputs": [],
   "source": [
    "from stable_baselines.common.policies import MlpLstmPolicy\n",
    "from stable_baselines.common.vec_env import DummyVecEnv\n",
    "from stable_baselines.ppo2 import PPO2\n",
    "from stable_baselines import bench, logger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "colab_type": "code",
    "id": "1fSNtJe_BQrf",
    "outputId": "a040d3a1-8a24-4577-d991-0162081f2f6e"
   },
   "outputs": [],
   "source": [
    "env = EquityEnv(use_cost=True)\n",
    "env = bench.Monitor(env, logger.get_dir(), allow_early_resets=True)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "model = PPO2(MlpLstmPolicy, env, n_steps=240, learning_rate=0.000001, verbose=0, nminibatches=1, policy_kwargs={\"n_lstm\":36, \"layers\":[36,36]}, tensorboard_log=\"./equity_train_tensorboard/\")\n",
    "print(\"-- Model Created --\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-- Training Model --\")\n",
    "model.learn(total_timesteps=35000)\n",
    "print(\"-- Finished --\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"PPO2_del\")\n",
    "print(\"-- Model Saved --\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reload Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "model = PPO2.load(\"../models/PPO2_model\")\n",
    "print(\"-- Model Loaded --\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 236
    },
    "colab_type": "code",
    "id": "9zVtSZMuKJuX",
    "outputId": "990dc6de-9bca-41a6-bc71-59176937c9e7"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def evaluate(model, environment, num_steps=30000):\n",
    "    pnl = []\n",
    "    dates = []\n",
    "    trans_cost = []\n",
    "    action_ls = []\n",
    "    obs = environment.reset()\n",
    "    \n",
    "    for i in range(num_steps):\n",
    "        if i % 5000 == 0:\n",
    "            print(i)\n",
    "            \n",
    "        # _states are only useful when using LSTM policies\n",
    "        action, _states = model.predict(obs, deterministic=True)\n",
    "        \n",
    "        # here, action, rewards and dones are arrays, because we are using vectorized env\n",
    "        obs, rewards, dones, info = environment.step(action)\n",
    "\n",
    "        # date format: 2018-10-30 09:30:00\n",
    "        date = datetime.strptime(info[0][\"date\"][\"Dates\"], '%Y-%m-%d %H:%M:%S')\n",
    "        cost = info[0][\"transaction_cost\"]\n",
    "       \n",
    "        # Stats\n",
    "        pnl.append(rewards[0])\n",
    "        dates.append(date)\n",
    "        trans_cost.append(cost)\n",
    "        action_ls.append(action)\n",
    "        if dones[0]:\n",
    "            break\n",
    "    \n",
    "    pnl = np.array(pnl)\n",
    "    dates = np.array(dates)\n",
    "    actions = np.array(action_ls)\n",
    "  \n",
    "    return pnl, dates, trans_cost, actions\n",
    "\n",
    "# Train reward before training\n",
    "pnl, dates, costs, actions = evaluate(model, env, num_steps=30000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import Formatter\n",
    "\n",
    "class MyFormatter(Formatter):\n",
    "    def __init__(self, dates, fmt='%Y-%m-%d'):\n",
    "        self.dates = dates\n",
    "        self.fmt = fmt\n",
    "\n",
    "    def __call__(self, x, pos=0):\n",
    "        'Return the label for time x at position pos'\n",
    "        ind = int(x)\n",
    "        if ind >= len(self.dates) or ind < 0:\n",
    "            return ''\n",
    "        else:\n",
    "            return self.dates[ind].strftime(self.fmt)\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "formatter = MyFormatter(dates)\n",
    "fig, ax = plt.subplots(figsize=(11, 7))\n",
    "ax.xaxis.set_major_formatter(formatter)\n",
    "ax.plot(np.arange(pnl.shape[0]), np.cumsum(pnl))\n",
    "fig.autofmt_xdate()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annual_sharpe(pnl):\n",
    "    mean = pnl.mean()\n",
    "    var = pnl.std()\n",
    "    day_sharpe = (mean / var) * np.sqrt(390)\n",
    "    year_sharpe = day_sharpe * np.sqrt(252)\n",
    "    return year_sharpe\n",
    "\n",
    "def annual_return(pnl, principal=1000000):    \n",
    "    ret = pnl / principal\n",
    "    return np.mean(ret) * 390 * 252\n",
    "\n",
    "def annual_volatility(pnl, principal=1000000):\n",
    "    log_ret = np.log(1 + pnl / principal)\n",
    "    return log_ret.std() * np.sqrt(252)\n",
    "\n",
    "def annual_turnover(weights):\n",
    "    turnover = np.sum(np.abs(weights[1:] - weights[:-1])) / weights.shape[0]\n",
    "    return turnover * 390 * 252\n",
    "\n",
    "def maximum_drawdown(pnl):\n",
    "    cum_pnl = np.cumsum(pnl)\n",
    "    ind = np.argmax(np.maximum.accumulate(cum_pnl) - cum_pnl)\n",
    "    return (np.maximum.accumulate(cum_pnl)[ind] - cum_pnl[ind]) / np.maximum.accumulate(cum_pnl)[ind]\n",
    "\n",
    "def print_statistics(pnl, actions):\n",
    "    print(\"Annual Sharpe: {}\".format(annual_sharpe(pnl)))\n",
    "    print(\"Annual Return: {}\".format(annual_return(pnl)))\n",
    "    print(\"Annual Volatility: {}\".format(annual_volatility(pnl)))\n",
    "    print(\"Annual Turnover: {}\".format(annual_turnover(actions)))\n",
    "    print(\"Maximum Drawdown: {}\".format(maximum_drawdown(pnl)))  \n",
    "    \n",
    "print_statistics(pnl, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "sb_main.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
